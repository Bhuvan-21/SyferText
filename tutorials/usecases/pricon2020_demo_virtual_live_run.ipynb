{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (PriCON2020 Demo) - SyferText_0.1.0\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:**\n",
    "\n",
    "- Alan Aboudib:  [Twitter](https://twitter.com/alan_aboudib) | [LinkedIn](https://www.linkedin.com/in/ala-aboudib/) | [Slack](https://app.slack.com/client/T6963A864/DDKH3SXKL/user_profile/UDKH3SH8S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1. The Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import syfertext\n",
    "from syfertext.tokenizer import Tokenizer\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "import torch\n",
    "hook = sy.TorchHook(torch) \n",
    "\n",
    "me = hook.local_worker\n",
    "me.is_client_worker = False\n",
    "\n",
    "bob = sy.VirtualWorker(hook, id = 'bob')\n",
    "alice = sy.VirtualWorker(hook, id = 'alice')\n",
    "charlie = sy.VirtualWorker(hook, id = 'charlie')\n",
    "dan = sy.VirtualWorker(hook, id = 'dan')\n",
    "bill = sy.VirtualWorker(hook, id = 'bill')\n",
    "james = sy.VirtualWorker(hook, id = 'james')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function print pipeline-related objects deployed\n",
    "def show_pipeline_objects(worker, pipeline_prefix):\n",
    "    \n",
    "    for k in list(worker._objects.keys()):\n",
    "        if str(k).startswith(pipeline_prefix):\n",
    "            print(str(k).ljust(40), '|  ', worker._objects[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to empty a worker's object store\n",
    "def reset_object_store(worker):\n",
    "    \n",
    "    keys = list(worker._objects)\n",
    "    for k in keys:\n",
    "        del worker._objects[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline using a Language object\n",
    "nlp_test1 = syfertext.create(pipeline_name = \"syfertext_pricon_test1\")\n",
    "\n",
    "# Create a tokenizer\n",
    "tokenizer = Tokenizer(suffixes=None,\n",
    "                      prefixes = None,\n",
    "                      infixes = ['_'],\n",
    "                      exceptions = {'manmade': [{'ORTH': 'man'},{'ORTH':'made'}]}\n",
    "                     )\n",
    "\n",
    "# Add the tokenizer to the pipeline\n",
    "nlp_test1.set_tokenizer(tokenizer = tokenizer,  \n",
    "                        access = {\"*\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 2. The Simple Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple tagger to tag tokens that are animal.\n",
    "from syfertext.pipeline import SimpleTagger\n",
    "\n",
    "animal_tagger = SimpleTagger(attribute = 'is_animal',\n",
    "                             lookups = ['dog', 'cat', 'horse', 'cow'],\n",
    "                             tag = True,\n",
    "                             default_tag = False,\n",
    "                             case_sensitive = False\n",
    "                          )\n",
    "\n",
    "# Add the animal tagger to the pipeline\n",
    "nlp_test1.add_pipe(name = 'animal tagger',\n",
    "                   component = animal_tagger,\n",
    "                   access = {'*'}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what heppened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer', 'animal tagger']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_test1.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_test1.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy it on PyGrid\n",
    "nlp_test1.deploy(worker = charlie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's tokenize some strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_object_store(me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syfertext_pricon_test1:tokenizer         |   State>None\n",
      "syfertext_pricon_test1:animal tagger     |   State>None\n",
      "syfertext_pricon_test1                   |   Pipeline>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(charlie, 'syfertext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pipeline_objects(me, 'syfertext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language>None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from the deployed version\n",
    "nlp_test1 = syfertext.load('syfertext_pricon_test1')\n",
    "nlp_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syfertext_pricon_test1                   |   Pipeline>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(me, 'syfertext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.generic.string import String\n",
    "\n",
    "text = String(\"I saw a dog playing with a manmade cat. A horse and a cow were there watching!\")\n",
    "\n",
    "doc = nlp_test1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syfertext_pricon_test1                   |   Pipeline>None\n",
      "syfertext_pricon_test1:tokenizer         |   State>None\n",
      "syfertext_pricon_test1:animal tagger     |   State>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(me, 'syfertext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "saw        | animal? False   | punct? False    | oov? True       |stopword? False\n",
      "a          | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "dog        | animal? True    | punct? False    | oov? True       |stopword? False\n",
      "playing    | animal? False   | punct? False    | oov? True       |stopword? False\n",
      "with       | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "a          | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "man        | animal? False   | punct? False    | oov? True       |stopword? False\n",
      "made       | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "cat        | animal? True    | punct? False    | oov? True       |stopword? False\n",
      ".          | animal? False   | punct? True     | oov? True       |stopword? False\n",
      "A          | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "horse      | animal? True    | punct? False    | oov? True       |stopword? False\n",
      "and        | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "a          | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "cow        | animal? True    | punct? False    | oov? True       |stopword? False\n",
      "were       | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "there      | animal? False   | punct? False    | oov? True       |stopword? True\n",
      "watching   | animal? False   | punct? False    | oov? True       |stopword? False\n",
      "!          | animal? False   | punct? True     | oov? True       |stopword? False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text.ljust(10), '|',\n",
    "          f\"animal? {token._.is_animal}\".ljust(15), '|',\n",
    "          f\"punct? {token.is_punct}\".ljust(15), '|',\n",
    "          f\"oov? {token.is_oov}\".ljust(15), '|'\n",
    "          f\"stopword? {token.is_stop}\".ljust(15),\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Now let's tokenize a remote string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syfertext.pointers.doc_pointer.DocPointer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = String(\"I saw a dog playing with a manmade cat. A horse and a cow were there watching!\")\n",
    "text = text.send(dan)\n",
    "\n",
    "doc = nlp_test1(text)\n",
    "\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syfertext_pricon_test1:tokenizer         |   State>None\n",
      "syfertext_pricon_test1:animal tagger     |   State>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(dan, 'syfertext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'me': [SubPipeline[tokenizer > animal tagger]],\n",
       " 'dan': [[SubPipelinePointer | me:7172858845 -> dan:50395814871]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_test1.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'me': [{'names': ['tokenizer', 'animal tagger'],\n",
       "               'location': 'me'}],\n",
       "             'dan': [{'names': ['tokenizer', 'animal tagger'],\n",
       "               'location': 'dan'}]})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_test1.subpipeline_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -3. Create a pipeline with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = syfertext.create(pipeline_name = \"spacy_en_core_web_lg_reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare vectors to create the vocabulary\n",
    "import pickle\n",
    "\n",
    "# Replace with pathes on your machine\n",
    "hash2row_path = '../../../language_models/syfertext_en_core_web_lg/syfertext_en_core_web_lg/data/key2row'\n",
    "vectors_path = '../../../language_models/syfertext_en_core_web_lg/syfertext_en_core_web_lg/data/vectors'\n",
    "\n",
    "# Load the key2row dict\n",
    "with open(hash2row_path, \"rb\") as hash2row_file:\n",
    "    hash2row = pickle.load(hash2row_file)\n",
    "    \n",
    "# Load the vectors\n",
    "with open(vectors_path, \"rb\") as vectors_file:\n",
    "    vectors = pickle.load(vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15748096671724166044: 303,\n",
       " 17130787002665352158: 197,\n",
       " 7803463073553212047: 4790,\n",
       " 2570228442999028682: 2,\n",
       " 7601031270107461210: 1,\n",
       " 2333086044707505052: 64,\n",
       " 17583811671891633967: 13004,\n",
       " 16386776449441186651: 153473,\n",
       " 17917450762234962061: 90212,\n",
       " 2845569251850464646: 4890,\n",
       " 6441753959142598228: 4589,\n",
       " 11219611539707024218: 13230,\n",
       " 6174446835319589640: 15533,\n",
       " 13726421824593857419: 20011,\n",
       " 1504634659258364126: 11242,\n",
       " 16574276769445059825: 223926,\n",
       " 7229085451700080888: 2103,\n",
       " 13398808308607858859: 12860,\n",
       " 10584754210639909977: 684823,\n",
       " 13533255182131191051: 15807,\n",
       " 15291897178858464945: 12234,\n",
       " 5645781836273801443: 74739,\n",
       " 14842797775714447761: 184394,\n",
       " 16296342147116913380: 49979,\n",
       " 13009980616664168188: 146852,\n",
       " 648685765859229059: 37622,\n",
       " 1532234788270237555: 138949,\n",
       " 3160290493673410775: 5778,\n",
       " 7192950948311231314: 55492,\n",
       " 898812733540616898: 74660,\n",
       " 5065724982986493403: 7029,\n",
       " 12145149956222337371: 11285,\n",
       " 2577432275152706222: 1305,\n",
       " 10614879281046276072: 31329,\n",
       " 1163180352375541156: 32634,\n",
       " 12000964256218870149: 194712,\n",
       " 16617080828790343937: 408878,\n",
       " 16451967036570147537: 340392,\n",
       " 12999135365122086734: 14090,\n",
       " 335912718137772646: 145668,\n",
       " 4715561826724714362: 12792,\n",
       " 10623909870718874235: 19755,\n",
       " 926220155893975456: 12,\n",
       " 11246769962773813969: 43,\n",
       " 7328857232392957309: 36,\n",
       " 1473888328299033804: 77,\n",
       " 12391408457559655798: 195,\n",
       " 4186076325264868450: 222,\n",
       " 10156466464828817018: 37,\n",
       " 13946165460493693086: 45,\n",
       " 12968460318509735395: 90,\n",
       " 17034714067226283036: 686,\n",
       " 14741448338680730870: 964,\n",
       " 1695318638798539832: 151,\n",
       " 5506140194365495296: 560,\n",
       " 1231651995299048479: 308,\n",
       " 16056574260218560434: 565,\n",
       " 4060566745642431527: 1252,\n",
       " 18300533323694220614: 1704,\n",
       " 16098657026865960786: 3174,\n",
       " 14616594366292570037: 4341,\n",
       " 10244713197478541316: 798,\n",
       " 6634784941664777789: 1570,\n",
       " 2199073406346111103: 18618,\n",
       " 11386672848716532786: 29,\n",
       " 3041798393981868370: 66215,\n",
       " 13886161490008922571: 192813,\n",
       " 2767453070306349719: 10,\n",
       " 11437511965130647350: 48716,\n",
       " 3933524689004914848: 5918,\n",
       " 14103661964789398356: 785,\n",
       " 11810352786104785236: 2150,\n",
       " 18370233690023496994: 42173,\n",
       " 10818040288739854079: 19383,\n",
       " 3491677490480064391: 3714,\n",
       " 3533345462595781991: 390,\n",
       " 15314171054814831536: 56,\n",
       " 16201970402572148383: 733,\n",
       " 4844570619372527965: 2311,\n",
       " 15327896488016134315: 156756,\n",
       " 8227948534797034094: 68,\n",
       " 10237029345517210811: 104,\n",
       " 13471060251654288974: 86,\n",
       " 5698858598239095446: 259,\n",
       " 16871188956558032961: 490,\n",
       " 3192048470772207689: 942,\n",
       " 6554146628057470732: 1169,\n",
       " 8891978952869636019: 626,\n",
       " 3770022288001954926: 162,\n",
       " 15788035189988467116: 13414,\n",
       " 14164987671898656280: 3666,\n",
       " 10951292870951308979: 1482,\n",
       " 1140705565974166848: 1810,\n",
       " 13732706168499917842: 1439,\n",
       " 9627129570676278632: 1155,\n",
       " 3496691256970997285: 4536,\n",
       " 16241160552541770025: 11407,\n",
       " 18117881399697484281: 22372,\n",
       " 6449376703685258820: 10814,\n",
       " 3465360207410646094: 15861,\n",
       " 7345169238154799948: 1838,\n",
       " 8068169394634657057: 5317,\n",
       " 579192984024245290: 57441,\n",
       " 4128016814358446831: 135,\n",
       " 742992508797545596: 209176,\n",
       " 2744064229254063911: 428756,\n",
       " 909527910512500163: 50,\n",
       " 8188876322239480300: 31622,\n",
       " 1608036226287564155: 6445,\n",
       " 18326987184532385976: 3033,\n",
       " 5242070432387461261: 148851,\n",
       " 2574426864888275505: 56496,\n",
       " 283776819636948959: 30705,\n",
       " 5587827918017164071: 3256,\n",
       " 9841548392764313153: 3334,\n",
       " 1078695049594320862: 15362,\n",
       " 700816980594137935: 33940,\n",
       " 6857681898833183959: 331877,\n",
       " 10512730944329424924: 1460,\n",
       " 2516671174952720084: 123,\n",
       " 4770100523811347367: 302,\n",
       " 7444888255207585720: 194,\n",
       " 715603177694545715: 283,\n",
       " 11775906505406399028: 166,\n",
       " 7960584982922761305: 305,\n",
       " 456214558858770980: 265,\n",
       " 14424204937981548578: 1762,\n",
       " 15186423388469697564: 1173,\n",
       " 6015756670082280225: 1548,\n",
       " 3507408723145969202: 2020,\n",
       " 17165898141669578209: 62968,\n",
       " 4687333233636294579: 1845,\n",
       " 11096999596672275808: 1720,\n",
       " 15506672634389823195: 2482,\n",
       " 12749515882577591084: 1356,\n",
       " 17565089786240495666: 1499,\n",
       " 12396372779537871939: 1955,\n",
       " 15802010742733194151: 1283,\n",
       " 15595990933209425089: 5315,\n",
       " 8577810079080564104: 2223,\n",
       " 14373744434911723683: 6554,\n",
       " 8783810227350449059: 7956,\n",
       " 18347933651302315841: 164164,\n",
       " 5943131912006430202: 4,\n",
       " 11815071289815375633: 47,\n",
       " 7577796991571031268: 9,\n",
       " 15207113440244469143: 49,\n",
       " 4573291672000494323: 114,\n",
       " 10554422762490591203: 124,\n",
       " 12603832552432291096: 116,\n",
       " 5775199995908356158: 70,\n",
       " 1829114629447710528: 229,\n",
       " 1088795901363325351: 35,\n",
       " 2751828473276665923: 69,\n",
       " 13769244734826917651: 603,\n",
       " 2889765388544198625: 93,\n",
       " 9882015785368280432: 7147,\n",
       " 8863031552414464448: 171,\n",
       " 12147819709780766541: 5158,\n",
       " 16539795004637876328: 1415,\n",
       " 9626604834594084717: 4575,\n",
       " 4857906525985111071: 348,\n",
       " 2500914552644716421: 358,\n",
       " 8739800065747573567: 891,\n",
       " 14993337645697103168: 2329,\n",
       " 13967061185506499492: 721,\n",
       " 11372458470609932510: 22047,\n",
       " 9502107258624682548: 2822,\n",
       " 15407257222220482149: 8523,\n",
       " 4709451656640161389: 592,\n",
       " 9424484833109902586: 1408,\n",
       " 14907937739174572263: 73,\n",
       " 8026747171847128095: 159,\n",
       " 3458972842292840964: 6401,\n",
       " 10989705549004382073: 333,\n",
       " 3164757995359516230: 2073,\n",
       " 6470772558030069695: 189,\n",
       " 9002346856241870504: 7140,\n",
       " 1544315481025800480: 146,\n",
       " 6588489156640524689: 11014,\n",
       " 644822967147054284: 2520,\n",
       " 8758383689677414157: 916,\n",
       " 15709649034204819197: 54362,\n",
       " 740811424123854673: 573,\n",
       " 17430642940322570313: 19859,\n",
       " 12481907724885794427: 11389,\n",
       " 2238347344543166825: 52063,\n",
       " 7252706431397633349: 16001,\n",
       " 14603059673004177793: 26615,\n",
       " 182519944749264420: 28671,\n",
       " 2708441614578433107: 137884,\n",
       " 8490954294239598293: 59854,\n",
       " 7973094395569938181: 95702,\n",
       " 3790326519984703195: 123976,\n",
       " 11642969259119640989: 89093,\n",
       " 17411663902371610775: 34,\n",
       " 13847793212785460718: 39,\n",
       " 8637148002480664534: 192,\n",
       " 3173401510781475738: 85,\n",
       " 13650671393796168828: 174,\n",
       " 13397206171804984350: 386,\n",
       " 7750428162972686928: 1597,\n",
       " 3892675454229386462: 1783,\n",
       " 5673053195782747712: 28,\n",
       " 9426046424776551329: 15,\n",
       " 6452383645288082571: 252,\n",
       " 5057888505690553100: 473372,\n",
       " 17807493589364706156: 231,\n",
       " 1722625390560423808: 480906,\n",
       " 4253734366550375172: 181976,\n",
       " 5441347997384912045: 472040,\n",
       " 17712050402254649458: 23,\n",
       " 7536569132734928797: 19,\n",
       " 15430884596617086542: 25,\n",
       " 17443000285297130197: 53,\n",
       " 1618373156726393262: 8,\n",
       " 11728213064939857863: 155,\n",
       " 6148388092923385661: 27,\n",
       " 10016575291538991581: 96,\n",
       " 7959507390074374949: 1640,\n",
       " 6999248049100471939: 679,\n",
       " 10379052414435503531: 212,\n",
       " 6072999628494611255: 941,\n",
       " 5723861558151687683: 795,\n",
       " 15707123867135637339: 2269,\n",
       " 11356261418201551321: 521,\n",
       " 18441238898356501263: 591856,\n",
       " 9886624581753078807: 101,\n",
       " 14051321037816772433: 3630,\n",
       " 1555476072322545163: 119,\n",
       " 15205921833025278131: 1662,\n",
       " 13706566957105553947: 11,\n",
       " 8488019955433563727: 71,\n",
       " 3310917309146703432: 14,\n",
       " 1305652835398897487: 76,\n",
       " 9142509015621756463: 88,\n",
       " 11721075771936434506: 337,\n",
       " 11875767913371022874: 546171,\n",
       " 266471892582541685: 2292,\n",
       " 16215045557044299892: 2885,\n",
       " 2361883413922826678: 656,\n",
       " 7314500124903094452: 55,\n",
       " 12592571832217103851: 145,\n",
       " 1243551699468197371: 592942,\n",
       " 15527237195099768020: 626220,\n",
       " 16942221900047362052: 26,\n",
       " 11458294468567680181: 102,\n",
       " 7171452748824691093: 82,\n",
       " 12364020635040391768: 84,\n",
       " 14184872349956345997: 1224,\n",
       " 1649771157704247618: 4212,\n",
       " 707688574185328313: 543,\n",
       " 4686490948588360845: 2290,\n",
       " 3315367747184154269: 1329,\n",
       " 1068157684214586734: 17,\n",
       " 17333296861998525656: 111,\n",
       " 15693692991445733772: 492725,\n",
       " 8764785459635682061: 540486,\n",
       " 6938841712554013421: 148,\n",
       " 596259668335416899: 34563,\n",
       " 9855993815404631269: 13542,\n",
       " 14477203068710363694: 48,\n",
       " 77802967464924628: 10602,\n",
       " 3313876046300232002: 4200,\n",
       " 12093155127339103715: 4098,\n",
       " 8753348953242377921: 130,\n",
       " 3095110091594993817: 216,\n",
       " 16245003942462969431: 149428,\n",
       " 6305030001287944198: 41,\n",
       " 18193369097811452391: 191687,\n",
       " 17700085778209130102: 22,\n",
       " 3975828871811736971: 8009,\n",
       " 2717267605663978886: 33,\n",
       " 9373422646279097699: 22863,\n",
       " 3894390131083956718: 1160,\n",
       " 271698644514489966: 242587,\n",
       " 17358648988891885945: 33524,\n",
       " 10052621057115702322: 35562,\n",
       " 11730150302829366384: 362,\n",
       " 15951080824650444886: 228,\n",
       " 12004768094793953433: 29735,\n",
       " 10973500593271867054: 5228,\n",
       " 5182201742351716208: 6,\n",
       " 18127279973184417664: 473399,\n",
       " 14921167250795587526: 20649,\n",
       " 994102482717204472: 3911,\n",
       " 8204022106892911254: 9549,\n",
       " 10035004318432562269: 2383,\n",
       " 16839840707921193467: 1628,\n",
       " 10968060317377467417: 925,\n",
       " 17106501326045553694: 5169,\n",
       " 3842970735906470934: 172,\n",
       " 11436533259571098204: 225,\n",
       " 3848507393718381222: 326,\n",
       " 14657356940261279062: 313,\n",
       " 3922014673247143766: 482,\n",
       " 14605089632925993138: 685,\n",
       " 16683377482312519056: 646,\n",
       " 9847323045718487286: 54423,\n",
       " 9723502875834141539: 172019,\n",
       " 12059746601540551715: 336579,\n",
       " 17376726275166787982: 1064,\n",
       " 11554538944830905183: 14023,\n",
       " 5739603225281446625: 237,\n",
       " 3065916622103662992: 2449,\n",
       " 17582981001154647169: 388141,\n",
       " 14130058524729730872: 12748,\n",
       " 8913458990249082618: 14495,\n",
       " 5012880691032281046: 26648,\n",
       " 14683716825919105622: 58951,\n",
       " 3987192428058164712: 21665,\n",
       " 14206655621806219841: 42794,\n",
       " 14619029971881138833: 139049,\n",
       " 9851873080688068228: 3473,\n",
       " 18108746661812288799: 638,\n",
       " 10639896064376422597: 1104,\n",
       " 2232108956477606786: 3013,\n",
       " 14129326614284972608: 9689,\n",
       " 9853906300711040298: 9108,\n",
       " 164334429962280154: 824,\n",
       " 5687884278966970250: 95,\n",
       " 14735271953164848252: 4974,\n",
       " 17609798613392981389: 9565,\n",
       " 9156225893426574532: 106548,\n",
       " 3591993151465088826: 447227,\n",
       " 16709395333468308160: 42294,\n",
       " 958863820798852668: 659,\n",
       " 8142824911051591316: 3619,\n",
       " 11320505456372453521: 265583,\n",
       " 6449170842722352542: 8870,\n",
       " 988757451943142683: 67281,\n",
       " 4779044950293613320: 504171,\n",
       " 10592671220849874096: 1213,\n",
       " 16688575096127623431: 186973,\n",
       " 13771156525379008389: 632221,\n",
       " 388666261000897422: 182,\n",
       " 18057366273588452988: 10477,\n",
       " 15081226752301450560: 2225,\n",
       " 3674337673509240168: 80620,\n",
       " 10763435192142434662: 3120,\n",
       " 13544457481535067926: 8337,\n",
       " 4561501219402224703: 202004,\n",
       " 16014857601065691470: 6883,\n",
       " 8312517500066881088: 473606,\n",
       " 13713336847115587676: 512247,\n",
       " 13623550082210759509: 128228,\n",
       " 18229458984613842249: 3816,\n",
       " 8898787460287322058: 554572,\n",
       " 2572227489563735898: 473174,\n",
       " 14437494595606047321: 148470,\n",
       " 6296437043710149756: 220555,\n",
       " 16405575370704018365: 6604,\n",
       " 907863912090193650: 672984,\n",
       " 9083116556497000817: 12895,\n",
       " 14133510114910376540: 14878,\n",
       " 10874593172980989772: 5500,\n",
       " 12328683937636675497: 590554,\n",
       " 16274280187113120473: 15623,\n",
       " 353965954744421247: 550436,\n",
       " 2989928250087561168: 1685,\n",
       " 582035129029371570: 8375,\n",
       " 8293595671858074726: 37142,\n",
       " 3181570354032088554: 2056,\n",
       " 10455553987426343773: 96682,\n",
       " 3005126053010353661: 2579,\n",
       " 17254951235455159525: 554606,\n",
       " 10067355054201451196: 1373,\n",
       " 13713574799433620132: 46,\n",
       " 17838684525299081392: 143,\n",
       " 15425184400723603688: 623,\n",
       " 11546471231827183433: 27703,\n",
       " 1676722871576301089: 17343,\n",
       " 1746171325594952234: 4843,\n",
       " 4829799117248176513: 473302,\n",
       " 9012307950733251546: 5123,\n",
       " 4609530440439329579: 18323,\n",
       " 11131810038517141865: 27835,\n",
       " 3909026348914634891: 159343,\n",
       " 10154874368846954533: 6014,\n",
       " 8195581525244377782: 664691,\n",
       " 10247859883189418437: 147192,\n",
       " 12155410900729868947: 17958,\n",
       " 3826063153241863845: 506768,\n",
       " 16819965145966601092: 42320,\n",
       " 3918350747841787910: 632931,\n",
       " 10372387783710126556: 1209,\n",
       " 7636949991101935794: 97,\n",
       " 17656690827702396543: 249430,\n",
       " 16757887439439258642: 334252,\n",
       " 5692156517589362413: 19055,\n",
       " 5740188515347659131: 477178,\n",
       " 10141331392921183830: 1679,\n",
       " 10239389836256687791: 473393,\n",
       " 6366101272011646661: 24180,\n",
       " 1772753429119384748: 3970,\n",
       " 7355317741553920998: 265453,\n",
       " 15651664636607940140: 17474,\n",
       " 8357088120775914855: 5116,\n",
       " 3814139603114728876: 563488,\n",
       " 1417144987344060212: 159556,\n",
       " 18059008541677163171: 21057,\n",
       " 7665896229717792383: 674818,\n",
       " 16538279767794754159: 623227,\n",
       " 10374582975731443593: 227197,\n",
       " 16620816767913576769: 59793,\n",
       " 1155499613899071230: 208,\n",
       " 14533535118979650747: 12654,\n",
       " 6427369687964434244: 195130,\n",
       " 5994646854147611801: 3629,\n",
       " 17140938604036887517: 627885,\n",
       " 11808915889823293453: 60143,\n",
       " 7563092105858060863: 2771,\n",
       " 15464375422364547458: 2363,\n",
       " 4084773552410090006: 15828,\n",
       " 1795614012082211945: 19025,\n",
       " 13789597568716002711: 6935,\n",
       " 14174722958813141413: 667861,\n",
       " 10635379923815965352: 3300,\n",
       " 3295459658495309125: 105684,\n",
       " 15160854440172495281: 3253,\n",
       " 17751432174519611562: 569286,\n",
       " 14941173456969948292: 96986,\n",
       " 7741331824571272797: 6417,\n",
       " 15025380360296820135: 578167,\n",
       " 10552636424020089750: 3675,\n",
       " 9498156536820782617: 41168,\n",
       " 4840235642042023278: 508373,\n",
       " 2970645244636752198: 59234,\n",
       " 5064815189449369052: 113,\n",
       " 12293954245130757049: 117,\n",
       " 632798674371419447: 12599,\n",
       " 2438054748369098236: 32756,\n",
       " 15598152509841740513: 21391,\n",
       " 17450663208990582245: 6186,\n",
       " 2848016049564825605: 5,\n",
       " 16659282039781321303: 1860,\n",
       " 7151351234264410269: 131,\n",
       " 13030684362737798818: 12102,\n",
       " 8154780027531789056: 3107,\n",
       " 9718910796079207437: 38572,\n",
       " 11698185392538419362: 570118,\n",
       " 15814829931621152700: 4976,\n",
       " 18395464791206560491: 21234,\n",
       " 1991233835898662299: 143536,\n",
       " 12331287746917262172: 258,\n",
       " 14401549865434810109: 38022,\n",
       " 2779208876946407828: 45355,\n",
       " 902578393182465585: 44,\n",
       " 4758147062373591632: 1422,\n",
       " 2050819213137342301: 16011,\n",
       " 8995403816008056854: 9833,\n",
       " 5729095110467730216: 545,\n",
       " 3791428037639250607: 473268,\n",
       " 12447498692643656233: 224074,\n",
       " 10319669162399398695: 274,\n",
       " 12452358893621602304: 87,\n",
       " 12126569146000111117: 1134,\n",
       " 7776173887780902514: 4826,\n",
       " 13349082131229970625: 473321,\n",
       " 11578881706845064180: 105,\n",
       " 17329841123575810096: 568,\n",
       " 15515407696736339255: 24208,\n",
       " 18301924526312998193: 2157,\n",
       " 4811263428564082379: 13818,\n",
       " 8139616277999264073: 9309,\n",
       " 14038629639937156736: 447226,\n",
       " 1362011420739029825: 15951,\n",
       " 1908979764362833004: 473278,\n",
       " 3800249312877378188: 115457,\n",
       " 10871730404318430133: 9315,\n",
       " 4326864060417825612: 643752,\n",
       " 12085993660886506158: 510,\n",
       " 9112848165044201410: 17009,\n",
       " 1776685897933524226: 569950,\n",
       " 12616141816139464669: 140723,\n",
       " 18308248721834274867: 7434,\n",
       " 540496143502008862: 482636,\n",
       " 9796388937433139044: 18699,\n",
       " 17937930935473538291: 4803,\n",
       " 12777160845956257429: 575131,\n",
       " 15030469306563962382: 5907,\n",
       " 6012012352932048450: 473248,\n",
       " 11409046085127119017: 9814,\n",
       " 5541094159255654529: 474623,\n",
       " 17837183842663612379: 211014,\n",
       " 9821043145585601851: 4294,\n",
       " 3323856447604740862: 467361,\n",
       " 7355134651509434849: 469633,\n",
       " 15151853541121778061: 162936,\n",
       " 1734575367452583034: 4430,\n",
       " 5079100442584526326: 470083,\n",
       " 516951735547575165: 236891,\n",
       " 13773951095912503743: 7770,\n",
       " 14836125793066767772: 550016,\n",
       " 15237266953286874632: 4127,\n",
       " 15706981229714302579: 136845,\n",
       " 2677434718749847724: 9879,\n",
       " 14708331172200433268: 535342,\n",
       " 7103574202535385974: 118485,\n",
       " 13453790755698205213: 11835,\n",
       " 1181560168425188124: 4463,\n",
       " 15874311736641531031: 729,\n",
       " 13603661148385075046: 254,\n",
       " 1292888628506560768: 49516,\n",
       " 11346043258380421834: 179,\n",
       " 11643633035249286487: 2801,\n",
       " 6912895308039181383: 27123,\n",
       " 10945227474538846391: 61003,\n",
       " 18381721727089743543: 605576,\n",
       " 11272304403455201645: 35143,\n",
       " 5037929241767151997: 25555,\n",
       " 8673369211372904119: 17438,\n",
       " 14660646617363616476: 3408,\n",
       " 10121222854775544057: 99148,\n",
       " 5506268204131646809: 4557,\n",
       " 13293208933928573984: 503815,\n",
       " 2101669551359911502: 473075,\n",
       " 7288219477074523378: 41346,\n",
       " 504835464236644530: 13373,\n",
       " 12152875396554676402: 563523,\n",
       " 5587973354923429564: 473177,\n",
       " 14357365641400928129: 2273,\n",
       " 17411278719541197817: 234,\n",
       " 10936256330465641610: 153935,\n",
       " 4379822773786932819: 506649,\n",
       " 8803633650747349822: 267618,\n",
       " 13892277986788045331: 551899,\n",
       " 4354748773768949988: 141734,\n",
       " 11372828061207732812: 5037,\n",
       " 14734810429631193229: 670085,\n",
       " 5757139614557694038: 662771,\n",
       " 12361745412137052508: 2564,\n",
       " 5641205937639686327: 201,\n",
       " 13092611756326222898: 301249,\n",
       " 15100313322453301895: 154058,\n",
       " 11624926020706079143: 7454,\n",
       " 13606688262934562807: 612448,\n",
       " 18258048457909136866: 53731,\n",
       " 6228255026866672320: 11995,\n",
       " 3392194021423823684: 669004,\n",
       " 11456084887251660328: 82667,\n",
       " 9192745936468126529: 676286,\n",
       " 13491989587875524461: 308557,\n",
       " 14784759082053417484: 559443,\n",
       " 9605121325281264983: 2558,\n",
       " 1818679504363386358: 566160,\n",
       " 3674489980903662340: 13248,\n",
       " 17074444128284978521: 660487,\n",
       " 6544502251452755674: 14481,\n",
       " 1511272333071057327: 612880,\n",
       " 13452787444316818306: 14714,\n",
       " 16805138488735218872: 12564,\n",
       " 11034712131289671537: 575929,\n",
       " 12543019644370029908: 4476,\n",
       " 5913917915706702731: 394,\n",
       " 5273046884522124939: 89586,\n",
       " 6418310960689055371: 93645,\n",
       " 11763553076448850758: 644433,\n",
       " 1944652225989164946: 4702,\n",
       " 2250008458598017857: 217424,\n",
       " 18046096032506331718: 606189,\n",
       " 18394146193913605799: 214954,\n",
       " 16846165766304559795: 625962,\n",
       " 4521192505451561634: 140181,\n",
       " 16812973998899216775: 616106,\n",
       " 10564398858402812419: 249166,\n",
       " 9944963215341831996: 632764,\n",
       " 11423714475950086774: 85259,\n",
       " 17830563019101044788: 679650,\n",
       " 9766498953025689424: 263651,\n",
       " 2228757288898131092: 12483,\n",
       " 15133261198734692973: 675130,\n",
       " 13871641614472060673: 472892,\n",
       " 9632249172763175488: 4180,\n",
       " 6585198958930203390: 133,\n",
       " 13180652523600914526: 216678,\n",
       " 15532167214611288369: 277443,\n",
       " 14191749139965028269: 13682,\n",
       " 11313378187016637828: 581130,\n",
       " 8284609171798295154: 39173,\n",
       " 16032316390530280040: 16927,\n",
       " 3013055297383307310: 70543,\n",
       " 10915794848870466154: 10160,\n",
       " 12294869344437716637: 5420,\n",
       " 7157973623855031071: 493376,\n",
       " 4359987553689041474: 118174,\n",
       " 11064034113843391560: 50895,\n",
       " 3270936148458553477: 107705,\n",
       " 16592866940709317076: 19411,\n",
       " 15499061683490766048: 2913,\n",
       " 1362060523793024155: 160710,\n",
       " 15439499987913832781: 22959,\n",
       " 1857652756527337611: 17225,\n",
       " 8401642410106151116: 152736,\n",
       " 5757644742493719059: 14185,\n",
       " 14682552945038207323: 150335,\n",
       " 3733964130274889366: 16415,\n",
       " 9523752697619567936: 24775,\n",
       " 3074354654917607700: 74170,\n",
       " 16792342762738066434: 18116,\n",
       " 16565575453524015762: 165893,\n",
       " 10248857669916282065: 11401,\n",
       " 12266727447983680143: 5463,\n",
       " 9213883173146068759: 660324,\n",
       " 6890179912026995: 473308,\n",
       " 13301344199043290969: 267505,\n",
       " 6188932928337454801: 9090,\n",
       " 18172827838304418464: 573828,\n",
       " 17401018375228719015: 32420,\n",
       " 18373585716601327783: 264,\n",
       " 12314601561097654169: 5434,\n",
       " 11909433367036047106: 5767,\n",
       " 9700913262523762567: 472390,\n",
       " 12842046331471903386: 87258,\n",
       " 10224605040597823977: 5510,\n",
       " 18133651546953084922: 447225,\n",
       " 13192584516256674797: 141015,\n",
       " 3721478991682417760: 90409,\n",
       " 14730948259378594395: 13438,\n",
       " 2891192617151271224: 657024,\n",
       " 4403578529665168440: 5645,\n",
       " 731506369299421681: 609898,\n",
       " 11856482629480277317: 654095,\n",
       " 5971002544677517041: 554052,\n",
       " 306188366175372094: 576951,\n",
       " 5743978468218826099: 472853,\n",
       " 1827620486421466989: 31472,\n",
       " 18301932560572863770: 623589,\n",
       " 12302300184799695754: 3205,\n",
       " 15241188621932829644: 473115,\n",
       " 5860188076546417104: 46837,\n",
       " 4511306066292658467: 680456,\n",
       " 7202578745637470167: 115110,\n",
       " 16752413354897305847: 617376,\n",
       " 12158890274278008938: 3664,\n",
       " 14470286571373417933: 37890,\n",
       " 4091654782914997071: 555955,\n",
       " 17501748500115897703: 121971,\n",
       " 17627283884004237918: 5491,\n",
       " 10784821517110632572: 561255,\n",
       " 17194465980791399112: 13356,\n",
       " 14989401451232551321: 502180,\n",
       " 11858622993060209721: 473291,\n",
       " 14016418741704298551: 49836,\n",
       " 6829640048949925890: 2385,\n",
       " 10376441882410124525: 42122,\n",
       " 9585746094189519524: 1957,\n",
       " 16422476206169929204: 109,\n",
       " 14736874066050453569: 99948,\n",
       " 11416093923967210693: 23341,\n",
       " 4153801433999617612: 4529,\n",
       " 4879161927072352527: 550224,\n",
       " 6266998224311941444: 133115,\n",
       " 3597725419524105159: 10258,\n",
       " 5349212476991676930: 567485,\n",
       " 18223948152852719918: 2422,\n",
       " 7141352741556108891: 2285,\n",
       " 13248598252128534961: 175,\n",
       " 15064341652746851372: 65,\n",
       " 53754034241743492: 45942,\n",
       " 9439368360035915047: 1633,\n",
       " 8337978676934106012: 296469,\n",
       " 1478977774196756527: 94375,\n",
       " 14186057679721709274: 3419,\n",
       " 7958387701345867010: 197303,\n",
       " 3887209049383298357: 116587,\n",
       " 13657456613647675826: 92751,\n",
       " 698095061578779340: 7070,\n",
       " 4008211751210294757: 479856,\n",
       " 13826807391646944819: 82961,\n",
       " 14091937347841359058: 3706,\n",
       " 5820733241428826673: 543896,\n",
       " 8283955560522522056: 3631,\n",
       " 11874233616791356050: 165,\n",
       " 2334577873894447514: 59,\n",
       " 573970139172476717: 296,\n",
       " 4721863822948186888: 72,\n",
       " 12609522116439022672: 26160,\n",
       " 14807883372815798406: 849,\n",
       " 5544947325535158135: 154,\n",
       " 15310900912931162942: 757,\n",
       " 7757671223697062713: 89,\n",
       " 2009471782621902010: 320,\n",
       " 11987251685809732588: 161,\n",
       " 15955377921082887141: 174044,\n",
       " 17619507392968413997: 7546,\n",
       " 9277198480728126329: 544939,\n",
       " 16994212574598011890: 531984,\n",
       " 15598139793587552062: 55654,\n",
       " 2207208210498975924: 312,\n",
       " 17263543572311032910: 933,\n",
       " 18135853142442439024: 3585,\n",
       " 8420378406057805574: 32077,\n",
       " 10790432296664530536: 71645,\n",
       " 13276111054825905062: 170800,\n",
       " 9922030862468820185: 13942,\n",
       " 9537569344822499548: 3320,\n",
       " 4773920796288612968: 32,\n",
       " 10021551701973162260: 7635,\n",
       " 15212026319416997609: 13272,\n",
       " 1311876220498222556: 5291,\n",
       " 17622749667514078161: 16832,\n",
       " 2421405413071644913: 27097,\n",
       " 3557961285425599989: 1833,\n",
       " 1133253733170681837: 16782,\n",
       " 15070583216025913768: 3021,\n",
       " 4681599978619388469: 19952,\n",
       " 804541190585564550: 3710,\n",
       " 3937991684116718466: 50771,\n",
       " 12594041730270944940: 473395,\n",
       " 4485711581619987499: 33053,\n",
       " 6165012802946768711: 28517,\n",
       " 7566896391736976424: 1700,\n",
       " 992378963999327104: 2650,\n",
       " 12898252803723826051: 50715,\n",
       " 14670165849125561394: 6515,\n",
       " 2772292821789438826: 38987,\n",
       " 18303571888911844496: 6426,\n",
       " 10789158829471725066: 63693,\n",
       " 4822367560670340742: 10209,\n",
       " 15614792792788180422: 30095,\n",
       " 14203093343687797818: 63985,\n",
       " 11416227174638579860: 13366,\n",
       " 13819895253362231760: 28776,\n",
       " 5241776568375922406: 5723,\n",
       " 18035438101579783775: 67401,\n",
       " 1560651853279502358: 36319,\n",
       " 1150772045097602188: 33926,\n",
       " 11708412542879939162: 26019,\n",
       " 12594190906856316260: 2009,\n",
       " 15702211378300764241: 64394,\n",
       " 1312194087455443878: 17165,\n",
       " 11925860080759683970: 473261,\n",
       " 6251605885960069277: 35853,\n",
       " 4024110151759363890: 49159,\n",
       " 6509029073406787746: 21704,\n",
       " 5952704252158236522: 153994,\n",
       " 5294415643656766984: 47564,\n",
       " 7125813880574601995: 24285,\n",
       " 14660106317849438788: 41852,\n",
       " 16218117761579546392: 7311,\n",
       " 14696613360017491858: 587073,\n",
       " 16621351741918660324: 31143,\n",
       " 5353933598763524627: 45658,\n",
       " 10748940092302293963: 25770,\n",
       " 17523733773600876136: 1143,\n",
       " 3163223970417668337: 8429,\n",
       " 15097116582672327621: 610062,\n",
       " 1529784589151094308: 2289,\n",
       " 15005119725733954353: 43894,\n",
       " 3786203002103567724: 7209,\n",
       " 8501532663918000446: 13185,\n",
       " 14003898563306986652: 238,\n",
       " 9213355212983669734: 187966,\n",
       " 12802023412023734113: 3429,\n",
       " 6983867020544837037: 147106,\n",
       " 4919573062460358712: 62259,\n",
       " 9357438155013644801: 671370,\n",
       " 12678237370847016782: 1404,\n",
       " 5413720037728270817: 51,\n",
       " 14808993546070024718: 39765,\n",
       " 6375788493953203230: 48183,\n",
       " 10446414560657320238: 9297,\n",
       " 5249361511563478927: 173350,\n",
       " 18311470978207731796: 99704,\n",
       " 6891582884955806469: 87215,\n",
       " 876396795818273589: 104192,\n",
       " 8764930570905850018: 4531,\n",
       " 6945422988897067813: 6232,\n",
       " 9801314206524109635: 447224,\n",
       " 2496585918639410020: 200384,\n",
       " 16539592244826601796: 447223,\n",
       " 7775430218048407468: 181852,\n",
       " 13448711137085732102: 3,\n",
       " 12923648954891151025: 7,\n",
       " 12927013974576775990: 13,\n",
       " 13714614600501292742: 16,\n",
       " 14714371505852328476: 18,\n",
       " 3231594134394939696: 20,\n",
       " 10466610102220986592: 21,\n",
       " 16915848774843609421: 24,\n",
       " 10010401745948606777: 78400,\n",
       " 16404382787692791222: 30,\n",
       " 8644371945174847701: 31,\n",
       " 1771193894125859972: 56441,\n",
       " 425069769939188373: 38,\n",
       " 13126640146197474015: 40,\n",
       " 12284404909592272887: 42,\n",
       " 11026518629776503584: 52,\n",
       " 11846258281147760795: 54,\n",
       " 12183995566920356821: 57,\n",
       " 5448391367153417939: 13878,\n",
       " 15731062810197717358: 58,\n",
       " 5148354600196274731: 60,\n",
       " 6600032844000000034: 61,\n",
       " 9524525335924992756: 62,\n",
       " 11679406961533675717: 188172,\n",
       " 13665603557740672776: 63,\n",
       " 9042334887352727675: 12988,\n",
       " 272617876247825538: 66,\n",
       " 8641477301663844378: 35809,\n",
       " 8966987516566847140: 67,\n",
       " 408547843991150828: 6725,\n",
       " 2591125827842313234: 1943,\n",
       " 9108059010810784004: 23435,\n",
       " 15722975925119728472: 74,\n",
       " 15237019812296040278: 75,\n",
       " 9507324864166986699: 101716,\n",
       " 11199852102921416062: 78,\n",
       " 12647799802415200240: 118,\n",
       " 12344959818088833377: 79,\n",
       " 17963354882297331722: 293365,\n",
       " 4951370564961824765: 80,\n",
       " 12140477091212017036: 81,\n",
       " 4387047730913993395: 85056,\n",
       " 5726087056488279436: 83,\n",
       " 8847650350893318144: 239771,\n",
       " 12040067090417805779: 10458,\n",
       " 14957297847906403895: 91,\n",
       " 15814918358424536147: 135182,\n",
       " 16383489737561091214: 92,\n",
       " 11267172639677495434: 181351,\n",
       " 14367437268289256764: 94,\n",
       " 10509855631966665095: 165360,\n",
       " 14611690243208358840: 98,\n",
       " 7953323516191625000: 99,\n",
       " 4531760108789613917: 100,\n",
       " 11337892499257938094: 132907,\n",
       " 12499319696624437388: 13435,\n",
       " 7472933747060301334: 103,\n",
       " 17401789145389008596: 106,\n",
       " 16569390047126526844: 107,\n",
       " 14234839353524236021: 13475,\n",
       " 1392980703560751535: 108,\n",
       " 10759867114511437557: 55202,\n",
       " 1463623539861555860: 110,\n",
       " 7297035564077993193: 112,\n",
       " 813507772558300698: 115,\n",
       " 16555784250675035861: 12181,\n",
       " 15125114607359950616: 120,\n",
       " 11108470328942452466: 121,\n",
       " 16883313871717912023: 72961,\n",
       " 13036791384442186643: 122,\n",
       " 630076102932587201: 9210,\n",
       " 15744020055604452424: 125,\n",
       " 9810037099978541979: 195397,\n",
       " 2469227116117951506: 126,\n",
       " 7354851929449012064: 92769,\n",
       " 11455037621131327031: 127,\n",
       " 4408465607537792596: 128,\n",
       " 6240097985076753180: 116940,\n",
       " 15141716136596633894: 129,\n",
       " 13883250732260620283: 132,\n",
       " 5800274158267776672: 72121,\n",
       " 17538946343191847414: 134,\n",
       " 1227945507479785602: 136,\n",
       " 14850927844002788960: 13262,\n",
       " 2895611376370838536: 137,\n",
       " 17949010229035489759: 49233,\n",
       " 3097737481060925969: 138,\n",
       " 7842781900626341462: 139,\n",
       " 2199640865840665977: 140,\n",
       " 10148488417912673814: 206351,\n",
       " 11991773498415570926: 141,\n",
       " 988374667740413748: 69346,\n",
       " 10534972467271895448: 142,\n",
       " 16035867470434130013: 144,\n",
       " 16317219331941319014: 147,\n",
       " 15851003424914253256: 65383,\n",
       " 12985017068911039343: 149,\n",
       " 14089912580555654184: 150,\n",
       " 2250347183682432037: 27096,\n",
       " 2639319884261285083: 7880,\n",
       " 1760647663911710998: 152,\n",
       " 4302169118448963797: 153,\n",
       " 9738469592818260845: 115158,\n",
       " 6726143786046315131: 156,\n",
       " 665819812834374203: 407723,\n",
       " 3273627595704613879: 157,\n",
       " 1518609443312062744: 66584,\n",
       " 5834040939753385979: 158,\n",
       " 601063019843420165: 160,\n",
       " 13944514515429733232: 197199,\n",
       " 8325414570413927404: 163,\n",
       " 17464466067038684192: 164,\n",
       " 15717804540086607331: 227,\n",
       " 2500643926280867251: 167,\n",
       " 6866371301200485824: 168,\n",
       " 17278678854790771590: 3423,\n",
       " 9261076352246603979: 169,\n",
       " 8542730283605594324: 170,\n",
       " 5199571586642613816: 6959,\n",
       " 7081829650383329907: 173,\n",
       " 12209254266871238630: 37018,\n",
       " 4721232070044793970: 176,\n",
       " 12102214764120363999: 26084,\n",
       " 14234935165153031517: 177,\n",
       " 1537531307324647143: 17705,\n",
       " 3992542033983125646: 178,\n",
       " 6302499412319283139: 83606,\n",
       " 6132239189403745484: 77510,\n",
       " 296366264361160138: 180,\n",
       " 14763034612516341022: 256172,\n",
       " 14729129442455071192: 181,\n",
       " 2858555193545982966: 494399,\n",
       " 854002071149538885: 183,\n",
       " 6376985215953038247: 7588,\n",
       " 10843067252759249924: 184,\n",
       " 3950902229651134655: 46249,\n",
       " 8220253382513974575: 185,\n",
       " 2822087240484491486: 227537,\n",
       " 17486883861226336625: 186,\n",
       " 7405909336905562021: 187,\n",
       " 3554473366614316316: 113308,\n",
       " 14128345243601234182: 188,\n",
       " 8685686345964238687: 190,\n",
       " 2147435734436461189: 191,\n",
       " 2554281855302796453: 580,\n",
       " 5322642585514818650: 193,\n",
       " 8071135352309062408: 90824,\n",
       " 18025944572464315732: 196,\n",
       " 11739300851648698792: 198,\n",
       " 4954664200181103830: 199,\n",
       " 7887613635506593076: 200,\n",
       " 5744669824186613314: 244913,\n",
       " 16151133519127949010: 202,\n",
       " 8988629591252936799: 7290,\n",
       " 15685982448842045781: 203,\n",
       " 10264473204794525105: 3204,\n",
       " 6874787943332331555: 204,\n",
       " 12956644141528873743: 434,\n",
       " 14501329860096752841: 205,\n",
       " 8838856022950177995: 206,\n",
       " 3526678628523148845: 36226,\n",
       " 11405224548381690417: 207,\n",
       " 15654172564955515935: 141128,\n",
       " 16129827305752939925: 209,\n",
       " 16047601784771673555: 210,\n",
       " 6489403369593428878: 211,\n",
       " 10103546308149463316: 3364,\n",
       " 11118993882393337800: 213,\n",
       " 10344977926825655930: 201306,\n",
       " 13789349970194523500: 214,\n",
       " 7157787116924106331: 215,\n",
       " 9120948653671069321: 217,\n",
       " 7707809753101326219: 218,\n",
       " 8595575128771878464: 219,\n",
       " 12974282963391685334: 3418,\n",
       " 9605378545343791713: 220,\n",
       " 8605548015844764670: 250083,\n",
       " 17196142852317255091: 221,\n",
       " 17409362612661898798: 209934,\n",
       " 2156471404464090308: 223,\n",
       " 9562782793769390347: 224,\n",
       " 17238239547704670293: 14464,\n",
       " 9636683445179514765: 226,\n",
       " 4749607083922092468: 83366,\n",
       " 8852035872903927508: 230,\n",
       " 9741529981488450657: 232,\n",
       " 16496229239704999207: 233,\n",
       " 12830358942201748691: 235,\n",
       " 224537444485558218: 167736,\n",
       " 943932539503151697: 236,\n",
       " 13027837389685897736: 158842,\n",
       " 12823293349314112591: 239,\n",
       " 4083094257850437443: 240,\n",
       " 12606982394142066508: 1199,\n",
       " 1331246166135331531: 241,\n",
       " 13775558749412955148: 242,\n",
       " 2160944286581786494: 117921,\n",
       " 8286069957012915224: 243,\n",
       " 6326283851605813033: 244,\n",
       " 320751236129934792: 185754,\n",
       " 17268578400025959152: 245,\n",
       " 2516695703746046006: 219639,\n",
       " 8958296314523169140: 246,\n",
       " 810362995247957494: 247,\n",
       " 4185449053370506607: 248,\n",
       " 8570312186860945105: 249,\n",
       " 8998673257964799256: 100840,\n",
       " 2345080743980033559: 250,\n",
       " 1935527660668822215: 425,\n",
       " 12163783820924860445: 473430,\n",
       " 2257536132728765972: 251,\n",
       " 16561193382610317443: 111614,\n",
       " 17875253539089781105: 253,\n",
       " 18387848742221898498: 1157,\n",
       " 9161947830747027411: 255,\n",
       " 16230951322598139115: 104198,\n",
       " 13279731795358054560: 256,\n",
       " 6381377732059037767: 257,\n",
       " 4021904456950951040: 260,\n",
       " 10861299944119434257: 28418,\n",
       " 4867931804126334991: 261,\n",
       " 4094223943394372819: 348475,\n",
       " 12993374122561765683: 262,\n",
       " 13806078475717176876: 108094,\n",
       " 10539202483782855665: 263,\n",
       " 9567280345707416518: 266,\n",
       " 41626951045607452: 267,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash2row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684831, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syfertext.vocab import Vocab\n",
    "\n",
    "# Create a Vocab object\n",
    "vocab = Vocab(hash2row = hash2row, vectors=vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the tokenizer and the vocabulary to the pipeline\n",
    "nlp_spacy.set_tokenizer(tokenizer = tokenizer, \n",
    "                        access = {'*'})\n",
    "\n",
    "nlp_spacy.set_vocab(vocab = vocab, \n",
    "                    access = {'*'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy\n",
    "nlp_spacy.deploy(worker=charlie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy_en_core_web_lg_reduced:tokenizer   |   State>None\n",
      "spacy_en_core_web_lg_reduced:vocab       |   State>None\n",
      "spacy_en_core_web_lg_reduced             |   Pipeline>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(charlie, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_object_store(me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "# Import useful utility functions for this tutoria\n",
    "from utils import download_dataset\n",
    "\n",
    "from syft.generic.string import String\n",
    "\n",
    "from syfertext.pipeline.simple_tagger import SimpleTagger\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "sb.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not run the following cell if you already have the dataset downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# The URL template to all dataset files\\nurl_template = 'https://raw.githubusercontent.com/AlanAboudib/dataset_imdb/master/%s'\\n\\n# File names to be downloaded from the using the URL template above\\nfiles = ['imdb.csv', 'imdb_vocab.txt', 'imdb_polarity.txt', 'stop_word_en.txt']\\n\\n# Construct the list of urls\\nurls = [url_template % file for file in files]\\n\\n# The dataset name and its root folder\\ndataset_name = 'imdb'\\nroot_path = './imdb'\\n\\n# Create the dataset folder if it is not already there\\nif not os.path.exists('./imdb'):\\n    os.mkdir('./imdb')\\n\\n# Start downloading\\ndownload_dataset(dataset_name = dataset_name, \\n                 urls = urls, \\n                 root_path = root_path\\n                )\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# The URL template to all dataset files\n",
    "url_template = 'https://raw.githubusercontent.com/AlanAboudib/dataset_imdb/master/%s'\n",
    "\n",
    "# File names to be downloaded from the using the URL template above\n",
    "files = ['imdb.csv', 'imdb_vocab.txt', 'imdb_polarity.txt', 'stop_word_en.txt']\n",
    "\n",
    "# Construct the list of urls\n",
    "urls = [url_template % file for file in files]\n",
    "\n",
    "# The dataset name and its root folder\n",
    "dataset_name = 'imdb'\n",
    "root_path = './imdb'\n",
    "\n",
    "# Create the dataset folder if it is not already there\n",
    "if not os.path.exists('./imdb'):\n",
    "    os.mkdir('./imdb')\n",
    "\n",
    "# Start downloading\n",
    "download_dataset(dataset_name = dataset_name, \n",
    "                 urls = urls, \n",
    "                 root_path = root_path\n",
    "                )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simulating Private Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to simulate two private datasets owned by two different clients, Bob and Alice. We will do the following:\n",
    "\n",
    "1. Load the whole dataset in `imdb.csv` locally (the `me` worker). This dataset will be loaded as a list of dictionaries that has the following format: `[ {'review': <review text>, 'label': <1 or 0>}, {...}, {...}]`\n",
    "\n",
    "\n",
    "2. Split the dataset into two parts, one for Bob and the other for Alice. Each part will be also split into a training set and a validation set. This will create four lists: `train_bob`, `valid_bob`, `train_alice`, `valid_alice`. Each list has the same format I mentioned above.\n",
    "\n",
    "\n",
    "3. Each element in the four lists will be sent to the corresponding worker. This will change the content of the lists as depicted in **Figure(1)**. Each list willl hold PySyft pointers to the texts and labels instead of the objects themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<img alt = 'imdb review remote datasets' src ='./art/imdb_review_remote.png' style='width:700px'>\n",
    "<div>\n",
    "<div style='width:600px;margin:30px auto 10px auto;text-align:center;'>\n",
    "<strong> Figure(1): </strong> The reviews and their labels are remotely located on Bob's and Alice's remote machines, only pointers to them are kept by the local worker (the company's machine).\n",
    "</div>\n",
    "</div>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a remote Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset file\n",
    "dataset_path = './imdb/imdb.csv'\n",
    "\n",
    "# store the dataset as a list of dictionaries\n",
    "# each dictionary has two keys, 'review' and 'label'\n",
    "# the 'review' element is a PySyft String\n",
    "# the 'label' element is an integer with 1 for 'positive'\n",
    "# and 0 for 'negative' review\n",
    "dataset_local = []\n",
    "\n",
    "with open(dataset_path, 'r') as dataset_file:\n",
    "    \n",
    "    # Create a csv reader object\n",
    "    reader = csv.DictReader(dataset_file)\n",
    "    \n",
    "    for elem in reader:\n",
    "        \n",
    "        # Create one entry\n",
    "        example = dict(review = String(elem['review']),\n",
    "                       label = 1 if elem['sentiment'] == 'positive' else 0\n",
    "                      )\n",
    "        \n",
    "        # add to the local dataset\n",
    "        dataset_local.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how an element in the list looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'review': 'Phil the Alien is one of those quirky films where the humour is based around the oddness of everything rather than actual punchlines.<br /><br />At first it was very odd and pretty funny but as the movie progressed I didn\\'t find the jokes or oddness funny anymore.<br /><br />Its a low budget film (thats never a problem in itself), there were some pretty interesting characters, but eventually I just lost interest.<br /><br />I imagine this film would appeal to a stoner who is currently partaking.<br /><br />For something similar but better try \"Brother from another planet\"'}\n"
     ]
    }
   ],
   "source": [
    "example = dataset_local[10]\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syft.generic.string.String'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(example['review']))\n",
    "print(type(example['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This review text is a PySyft `String` object. The label is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into two equal parts and send each part to a different worker simulating two remote datasets as I mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two datasets, one for Bob, and the other for Alice\n",
    "\n",
    "dataset_bob, dataset_alice = train_test_split(dataset_local[:250], train_size = 0.5)\n",
    "\n",
    "# Now create a validation set for Bob, and another for Alice\n",
    "train_bob, val_bob = train_test_split(dataset_bob, train_size = 0.7)\n",
    "train_alice, val_alice = train_test_split(dataset_alice, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now I will make the dataset remote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that sends the content of each split to a remote worker\n",
    "def make_remote_dataset(dataset, worker):\n",
    "\n",
    "    # Got through each example in the dataset\n",
    "    for example in dataset:\n",
    "        \n",
    "        # Send each review text\n",
    "        example['review'] = example['review'].send(worker)\n",
    "\n",
    "        # Send each label as a one-hot-enceded vector\n",
    "        one_hot_label = torch.zeros(2).scatter(0, torch.Tensor([example['label']]).long(), 1)\n",
    "        \n",
    "        # Send the review label\n",
    "        example['label'] = one_hot_label.send(worker)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the above function, transforms the label to a one-hot-encoded format before sending it to a remote worker. So if the sentiment is negative, the corresponding tensor will hold `[1,0]`, and if it is positive, the label will be `[0,1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can finally create the remote datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob's remote dataset\n",
    "make_remote_dataset(train_bob, bob)\n",
    "make_remote_dataset(val_bob, bob)\n",
    "\n",
    "# Alice's remote dataset\n",
    "make_remote_dataset(train_alice, alice)\n",
    "make_remote_dataset(val_alice, alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you what an element of Bob's dataset look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syft.generic.pointers.string_pointer.StringPointer'>\n",
      "(Wrapper)>[PointerTensor | me:70409470041 -> bob:14429565514]\n"
     ]
    }
   ],
   "source": [
    "# Take an element from the dataset\n",
    "example = train_bob[2]\n",
    "\n",
    "print(type(example['review']))\n",
    "print(example['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the text type is now a PySyft `StringPointer` that points to the real `String` object  located in Bob's machine. The label type is a PySyft `PointerTensor`. Let's check out the location of the real text and label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:bob #objects:250>\n",
      "<VirtualWorker id:bob #objects:250>\n"
     ]
    }
   ],
   "source": [
    "print(example['review'].location)\n",
    "print(example['label'].location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are now ready, and so is the work environment. Let's start the fun with SyferText :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the new pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an existing pipeline from PyGrid\n",
    "nlp_spacy = syfertext.load('spacy_en_core_web_lg_reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenizer': {'state': State>None, 'access': {'*'}, 'location_id': 'charlie'},\n",
       " 'vocab': {'state': State>None, 'access': {'*'}, 'location_id': 'charlie'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_spacy.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I          | punct? False    | oov? False      |stopword? True\n",
      "saw        | punct? False    | oov? False      |stopword? False\n",
      "a          | punct? False    | oov? False      |stopword? True\n",
      "dog        | punct? False    | oov? False      |stopword? False\n",
      "playing    | punct? False    | oov? False      |stopword? False\n",
      "with       | punct? False    | oov? False      |stopword? True\n",
      "a          | punct? False    | oov? False      |stopword? True\n",
      "manmade    | punct? False    | oov? False      |stopword? False\n",
      "cat        | punct? False    | oov? False      |stopword? False\n",
      ".          | punct? True     | oov? False      |stopword? False\n",
      "A          | punct? False    | oov? False      |stopword? True\n",
      "horse      | punct? False    | oov? False      |stopword? False\n",
      "and        | punct? False    | oov? False      |stopword? True\n",
      "a          | punct? False    | oov? False      |stopword? True\n",
      "cow        | punct? False    | oov? False      |stopword? False\n",
      "were       | punct? False    | oov? False      |stopword? True\n",
      "there      | punct? False    | oov? False      |stopword? True\n",
      "watching   | punct? False    | oov? False      |stopword? False\n",
      "!          | punct? True     | oov? False      |stopword? False\n"
     ]
    }
   ],
   "source": [
    "# Let's tokenize with the imported model\n",
    "doc = nlp_spacy(\"I saw a dog playing with a manmade cat. A horse and a cow were there watching!\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text.ljust(10), '|',\n",
    "          f\"punct? {token.is_punct}\".ljust(15), '|',\n",
    "          f\"oov? {token.is_oov}\".ljust(15), '|'\n",
    "          f\"stopword? {token.is_stop}\".ljust(15),\n",
    "         )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded model has word embeddings (that we are going to use for our usecase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8733e-01,  4.0595e-01, -5.1174e-01, -5.5482e-01,  3.9716e-02,\n",
       "          1.2887e-01,  4.5137e-01, -5.9149e-01,  1.5591e-01,  1.5137e+00,\n",
       "         -8.7020e-01,  5.0672e-02,  1.5211e-01, -1.9183e-01,  1.1181e-01,\n",
       "          1.2131e-01, -2.7212e-01,  1.6203e+00, -2.4884e-01,  1.4060e-01,\n",
       "          3.3099e-01, -1.8061e-02,  1.5244e-01, -2.6943e-01, -2.7833e-01,\n",
       "         -5.2123e-02, -4.8149e-01, -5.1839e-01,  8.6262e-02,  3.0818e-02,\n",
       "         -2.1253e-01, -1.1378e-01, -2.2384e-01,  1.8262e-01, -3.4541e-01,\n",
       "          8.2611e-02,  1.0024e-01, -7.9550e-02, -8.1721e-01,  6.5621e-03,\n",
       "          8.0134e-02, -3.9976e-01, -6.3131e-02,  3.2260e-01, -3.1625e-02,\n",
       "          4.3056e-01, -2.7270e-01, -7.6020e-02,  1.0293e-01, -8.8653e-02,\n",
       "         -2.9087e-01, -4.7214e-02,  4.6036e-02, -1.7788e-02,  6.4990e-02,\n",
       "          8.8451e-02, -3.1574e-01, -5.8522e-01,  2.2295e-01, -5.2785e-02,\n",
       "         -5.5981e-01, -3.9580e-01, -7.9849e-02, -1.0933e-02, -4.1722e-02,\n",
       "         -5.5576e-01,  8.8707e-02,  1.3710e-01, -2.9873e-03, -2.6256e-02,\n",
       "          7.7330e-02,  3.9199e-01,  3.4507e-01, -8.0130e-02,  3.3451e-01,\n",
       "          2.7063e-01, -2.4544e-02,  7.2576e-02, -1.8120e-01,  2.3693e-01,\n",
       "          3.9977e-01,  4.5012e-01,  2.7179e-02,  2.7400e-01,  1.4791e-01,\n",
       "         -5.8324e-03,  9.5910e-01, -1.0129e+00,  2.0699e-01,  1.8237e-01,\n",
       "         -2.5234e-01, -2.6261e-01, -3.4799e-01, -2.4051e-02,  4.4470e-01,\n",
       "          5.9226e-02,  4.5561e-01,  1.9700e-01, -4.8327e-01,  8.9523e-02,\n",
       "         -2.2373e-01, -1.5654e-01,  2.1578e-01,  1.1673e-01,  8.2006e-02,\n",
       "         -8.0735e-01,  2.3903e-01, -5.1304e-01, -3.3888e-01, -3.1499e-01,\n",
       "         -1.7272e-01, -6.7020e-01,  2.7096e-01, -4.3241e-01,  4.3103e-02,\n",
       "          2.1233e-02,  1.3350e-02, -6.3938e-02, -2.4957e-01, -2.4938e-01,\n",
       "          3.4812e-01, -7.1321e-02,  2.3375e-01, -9.5384e-02,  5.2488e-01,\n",
       "          6.8175e-01, -1.0214e-01, -1.4914e-01, -7.5697e-02,  1.7248e-01,\n",
       "          2.5440e-01,  1.5760e-01, -5.9125e-01,  2.4300e-01,  6.3962e-01,\n",
       "         -9.3280e-02, -2.7914e-01, -6.6262e-02, -6.7170e-02, -4.0929e-01,\n",
       "         -3.0300e+00,  1.8250e-01,  2.0113e-01,  6.0628e-02, -2.4769e-01,\n",
       "          5.5324e-02, -4.9106e-01,  3.1544e-01, -3.4231e-01, -6.3766e-01,\n",
       "         -3.6129e-01, -5.9029e-02,  1.5510e-01,  4.4577e-02,  2.3572e-01,\n",
       "         -1.7095e-01, -2.2749e-01, -2.3184e-02,  2.3868e-01,  2.8170e-02,\n",
       "          4.2965e-01, -1.2458e-01, -3.6972e-02,  2.0061e-01, -3.1405e-01,\n",
       "         -8.5287e-02, -3.3496e-01, -9.7047e-02, -1.4388e-01,  1.1147e-01,\n",
       "         -4.5232e-01, -2.4217e-01, -1.8245e-01, -6.7292e-01,  2.1933e-02,\n",
       "         -5.4816e-02, -4.6508e-01,  4.7767e-01, -2.4752e-01, -1.5790e-01,\n",
       "          1.1817e-01,  5.6851e-02, -4.9151e-01,  1.5496e-01,  1.6425e-02,\n",
       "          4.1650e-02, -3.4990e-01, -1.5979e-01,  3.9705e-01,  2.2963e-01,\n",
       "          2.4688e-01,  1.9567e-02, -2.8802e-01, -6.9983e-01,  3.2744e-01,\n",
       "          1.0833e-01,  2.4945e-01, -7.8653e-01, -6.1379e-02, -3.7359e-01,\n",
       "         -1.1603e-01, -2.4950e-01,  1.0161e-01,  3.3994e-02,  1.5650e-01,\n",
       "          2.1344e-01, -1.1094e-01, -5.7687e-03,  1.7869e-01, -1.0127e-01,\n",
       "         -1.6891e-02,  3.0001e-01, -3.4116e-01, -3.2390e-02,  4.2514e-02,\n",
       "          1.1850e-01, -1.8337e-01, -6.2865e-01, -2.8021e-01,  4.2351e-01,\n",
       "          1.1277e-01,  1.2121e-03,  1.5710e-01, -3.6321e-01, -4.9251e-01,\n",
       "          1.1653e-01,  2.4024e-01,  1.7712e-01,  6.8700e-02, -4.4137e-01,\n",
       "         -2.9877e-01, -1.2071e-02,  2.8325e-01,  1.0668e-01, -1.8859e-01,\n",
       "         -4.1345e-01, -3.4090e-01,  4.7236e-02, -3.8309e-01,  4.3572e-01,\n",
       "          2.4505e-01,  2.7337e-01, -7.3038e-02,  4.2514e-01, -3.2455e-02,\n",
       "         -3.5211e-01,  4.5691e-01,  1.9433e-01, -1.5230e-01,  4.2675e-01,\n",
       "          2.8795e-01, -5.5969e-01, -1.3031e-01,  8.9844e-02,  4.2605e-01,\n",
       "         -1.9632e-01, -7.1989e-02, -8.0189e-02, -3.0425e-01, -4.6190e-01,\n",
       "          2.8178e-01, -9.9872e-02,  3.5097e-01,  1.6123e-01, -3.6548e-02,\n",
       "         -3.6739e-01, -1.9819e-02,  3.2130e-01,  1.7479e-01,  2.5175e-01,\n",
       "         -7.6439e-03, -9.3786e-02, -3.7852e-01,  4.3725e-01,  2.1288e-01,\n",
       "          2.5096e-01, -1.9613e-01, -2.8865e-01, -5.6726e-03,  4.2795e-01,\n",
       "          2.0625e-01, -3.7701e-02, -1.2200e-01, -7.9253e-02, -1.0290e-01,\n",
       "          1.0558e-02,  4.9880e-01,  2.5382e-01,  1.5526e-01,  1.7951e-03,\n",
       "          1.1633e-01,  7.9300e-02, -3.9142e-01, -3.2483e-01,  6.3451e-01,\n",
       "         -1.8910e-01,  5.4050e-02,  1.6495e-01,  1.8757e-01,  5.3874e-01]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the new pipeline\n",
    "nlp_sentiment = syfertext.create('syfertext_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "nlp_sentiment.set_tokenizer(tokenizer = tokenizer, \n",
    "                            access= {'*'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add a vocab from 'spacy_en_core_web_lg_reduced'\n",
    "vocab_spacy = nlp_spacy('').vocab\n",
    "\n",
    "nlp_sentiment.set_vocab(vocab=vocab_spacy, access={'*'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenizer': {'state': State>None, 'access': {'*'}},\n",
       " 'vocab': {'state': State>None, 'access': {'*'}}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment.states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a  tagger for stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating the stop-word tagger. Let's first load the stop word file into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from syfertext.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the tagger which is an object of the `SimpleTagger` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple tagger object to tag stop words\n",
    "stop_tagger = SimpleTagger(attribute = 'is_stop',\n",
    "                           lookups = STOP_WORDS,\n",
    "                           tag = True,\n",
    "                           default_tag = False,\n",
    "                           case_sensitive = False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I pass the list of words as the `lookups` arguments. \n",
    "\n",
    "Every token in the `Doc` object will be given a custom attribute called `is_stop`. Every time a stop word is found, this attribute will be given the value `True` specified by the `tag` argument of the `SimpleTagger` class initialiser, otherwise, the `default_tag` will be used, which I set to `False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create a tagger for word polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we created a tagger for stop words. We are now going to create another tagger for polar words, i.e., words that are more biased toward a positive or a negative sentiment. Let's load the corresponding files `imdb_vocab.txt` and `imdb_polarity.txt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the polarity info\n",
    "with open('./imdb/imdb_vocab.txt', 'r') as f:\n",
    "    imdb_words = f.read().splitlines()\n",
    "    \n",
    "with open('./imdb/imdb_polarity.txt', 'r') as f:\n",
    "    polarity = [float(line) for line in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me show you the distribution of polarity values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/miniconda3/envs/syfertext/lib/python3.7/site-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the histogram of polarity values\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "sb.distplot(polarity, kde = False, ax = ax)\n",
    "\n",
    "ax.set_xlabel('Sentiment Polarity Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(\"Distribution of Polarity Values in the IMDB dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the grand majority of words seem to be unbiased toward a specific sentiment. So let's create a tagger that tags only tokens that are most polar by setting a custom attribute we will call `is_polar` to `True` and `False` otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose low/high polarity cutoff values\n",
    "low_cutoff = -0.5\n",
    "high_cutoff = 0.5\n",
    "\n",
    "# Create a list of polar tokens\n",
    "polar_tokens = [token for i, token in enumerate(imdb_words) \n",
    "                if polarity[i] > high_cutoff or\n",
    "                polarity[i] < low_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the list of polar wordsabove, we can now create the tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_tagger = SimpleTagger(attribute = 'is_polar',\n",
    "                               lookups = polar_tokens,\n",
    "                               tag = True,\n",
    "                               default_tag = False,\n",
    "                               case_sensitive = False\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Adding the taggers to the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add each tagger we created above to the the pipeline by using the `add_pipe()` method of the `Language` class. However, in the following cell, I give you the possibility to decide for yourself which components you wish to add.\n",
    "\n",
    "Here is what I recommend you do:\n",
    "\n",
    "1. First run this tutorial without adding an tagger.\n",
    "\n",
    "2. Restart the notebook and run the tutorial again with `use_stop_tagger = True`.\n",
    "\n",
    "3. Restart the notebook and run the tutorial again with both `use_stop_tagger = True` and `use_polarity_tagger = True`.\n",
    "\n",
    "I will actually show you the results of each such run at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stop_tagger = True\n",
    "use_polarity_tagger = True\n",
    "\n",
    "# Tokens with these custom tags\n",
    "# will be excluded from creating\n",
    "# the Doc vector\n",
    "excluded_tokens = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the above cell. I create a dictionary called `excluded_tokens`. It will be used later in this tutorial when we create embedding vectors for reviews. It enables us to execlude some tokens when we create a document embedding. Such exclusion will be based on the value of the custom attributes we set with the taggers.\n",
    "\n",
    "Now let's add the stop word tagger to the pipeline (If `use_stop_tagger = True`). Notice that I set the argument `remote = True`. This tells the `Language` object that it is allowed to send the pipe component to the remote worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_stop_tagger:\n",
    "\n",
    "    # Add the stop word to the pipeline\n",
    "    nlp_sentiment.add_pipe(name = 'stop tagger',\n",
    "                 component = stop_tagger,\n",
    "                 access = {'*'}\n",
    "                )\n",
    "\n",
    "    # Tokens with 'is_stop' = True are\n",
    "    # not going to be used when creating the \n",
    "    # Doc vector\n",
    "    excluded_tokens['is_stop'] = {True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for adding the polar word tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_polarity_tagger:\n",
    "    \n",
    "    # Add the polarity tagger to the pipeline\n",
    "    nlp_sentiment.add_pipe(name = 'polarity tagger',\n",
    "                 component = polarity_tagger,\n",
    "                 access = {'*'}\n",
    "                )\n",
    "\n",
    "    # Tokens with 'is_polar' = False are\n",
    "    # not going to be used when creating the \n",
    "    # Doc vector\n",
    "    excluded_tokens['is_polar'] = {False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what pipe components are included in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tokenizer', 'class_name': 'Tokenizer'},\n",
       " {'name': 'stop tagger', 'class_name': 'SimpleTagger'},\n",
       " {'name': 'polarity tagger', 'class_name': 'SimpleTagger'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment.pipeline_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some necessary imports\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a summary writer for logging performance with Tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetIMDB(Dataset):\n",
    "    \n",
    "    def __init__(self, sets, share_workers, crypto_provider, nlp):\n",
    "\n",
    "        self.sets = sets\n",
    "        self.crypto_provider = crypto_provider\n",
    "        self.workers = share_workers\n",
    "    \n",
    "        # Create a single dataset unifying all datasets.\n",
    "        self._create_dataset()\n",
    "        \n",
    "        # The language model\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def _create_dataset(self):\n",
    "        \"\"\"Create a single list unifying examples from all remote datasets\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the dataset\n",
    "        self.dataset = []\n",
    "      \n",
    "        # Populate the dataset list\n",
    "        for dataset in self.sets:\n",
    "            for example in dataset:\n",
    "                self.dataset.append(example)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "        # get the example\n",
    "        example = self.dataset[index]\n",
    "        \n",
    "        # Run the preprocessing pipeline on \n",
    "        # the review text and get a DocPointer object\n",
    "        doc_ptr = self.nlp(example['review'])\n",
    "        \n",
    "        # Get the encrypted vector embedding for the document\n",
    "        vector_enc = doc_ptr.get_encrypted_vector(bob, \n",
    "                                                  alice, \n",
    "                                                  crypto_provider = self.crypto_provider,\n",
    "                                                  requires_grad = True,\n",
    "                                                  excluded_tokens = excluded_tokens,\n",
    "                                                  protocol = 'fss'\n",
    "                                                 )\n",
    "        \n",
    "\n",
    "        # Encrypt target label\n",
    "        label_enc = example['label'].fix_precision()\\\n",
    "                                    .share(bob, \n",
    "                                           alice, \n",
    "                                           crypto_provider = self.crypto_provider,\n",
    "                                           requires_grad = True,\n",
    "                                           protocol = 'fss'\n",
    "\n",
    "                                          )\\\n",
    "                                    .get()\n",
    "\n",
    "\n",
    "        return vector_enc, label_enc\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the combined size of all of the \n",
    "        remote training/validation sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The size of the combined datasets\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "                \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"The collat_fn method to be used by the\n",
    "        PyTorch data loader.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unzip the batch\n",
    "        vectors, targets = list(zip(*batch))\n",
    "\n",
    "        # concatenate the vectors\n",
    "        vectors = torch.stack(vectors)\n",
    "        \n",
    "        #concatenate the labels\n",
    "        targets = torch.stack(targets)\n",
    "        \n",
    "        return vectors, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create two such `DatasetIMDB` objects, one for training and the other for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a training Dataset object\n",
    "trainset = DatasetIMDB(sets = [train_bob,\n",
    "                               train_alice],\n",
    "                       share_workers = [bob, alice],\n",
    "                       crypto_provider = dan,\n",
    "                       nlp = nlp_sentiment\n",
    "                      )\n",
    "\n",
    "# Instantiate a validation Dataset object\n",
    "valset = DatasetIMDB(sets = [val_bob,\n",
    "                             val_alice],\n",
    "                     share_workers = [bob, alice],\n",
    "                     crypto_provider = dan,\n",
    "                     nlp = nlp_sentiment\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create a DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now choose some hyper parameters for training and validation, and create the PyTorch data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some hyper parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DataLoader object for the training set\n",
    "trainloader = DataLoader(trainset, \n",
    "                         shuffle = True,\n",
    "                         batch_size = batch_size, \n",
    "                         num_workers = 0, \n",
    "                         collate_fn = trainset.collate_fn)\n",
    "\n",
    "\n",
    "# Instantiate the DataLoader object for the validation set\n",
    "valloader = DataLoader(valset, \n",
    "                       shuffle = True,\n",
    "                       batch_size = batch_size, \n",
    "                       num_workers = 0, \n",
    "                       collate_fn = valset.collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create an Encrypted Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment classifier I use here is simply a linear layer with `300` input features which is the size of the embedding vectors computed by SyferText. A ReLU activation is then applied. The network has two outputs, one for negative sentiments and the other for positive ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "\n",
    "class Classifier(sy.Plan):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc = torch.nn.Linear(in_features, out_features)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        probs = F.relu(logits)\n",
    "        \n",
    "        return probs, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should now initialize and encrypt the classifier. Encryption here should of course use the same workers to hold the shares and the same primitives used to encrypt the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifer\n",
    "classifier = Classifier(in_features = 300, out_features = 2)\n",
    "\n",
    "# Apply SMPC encryption\n",
    "classifier = classifier.fix_precision()\\\n",
    "                       .share(bob, alice, \n",
    "                              crypto_provider = dan,\n",
    "                              requires_grad = True,\n",
    "                              protocol = 'fss'\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally I create an optimizer. Notice that the optimizer does not need to be encrypted, since it operates separately within each worker holding the classifier's and embeddings' shares. We just need to make it operate on fixed precision numbers that are used to encode shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optim.SGD(params = classifier.parameters(),\n",
    "                  lr = learning_rate)\n",
    "\n",
    "optim = optim.fix_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo!!! You are now ready to launch training. \n",
    "\n",
    "Notice that we use MSE as a training loss which is not the best choice for a classification task. I choose to use it since the `NLLLoss()` is not yet implemented in PySyft for SMPC mode. But it is an issue that is currently being worked on.\n",
    "\n",
    "In order to view the training and validation curves for loss and accuracy, you need to run `Tensorboard`. Just open a terminal, navigate to the folder containing this notebook, and run:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir runs/\n",
    "```\n",
    "\n",
    "Then open your favorite web browser and go to `localhost:6006`.\n",
    "\n",
    "The below cell will produce no outputs. But you be able to see performance curves on Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-c062504472ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Set train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.7/site-packages/torch-1.4.0-py3.7-linux-x86_64.egg/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.7/site-packages/torch-1.4.0-py3.7-linux-x86_64.egg/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.7/site-packages/torch-1.4.0-py3.7-linux-x86_64.egg/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/syfertext/lib/python3.7/site-packages/torch-1.4.0-py3.7-linux-x86_64.egg/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-050db99ded27>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Run the preprocessing pipeline on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# the review text and get a DocPointer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mdoc_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Get the encrypted vector embedding for the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;31m# Create a subpipeline templates list for the worker where `input` is located\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# If it does not already exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_pipeline_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_owner_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_owner_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# Runs the first subpipeline.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/language.py\u001b[0m in \u001b[0;36m_parse_pipeline_template\u001b[0;34m(self, data_owner_id)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubpipeline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_owner_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0msubpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_or_create_subpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSubPipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/pipeline/pointers/subpipeline_pointer.py\u001b[0m in \u001b[0;36mload_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Send the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         self.owner.send_command(\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mrecipient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"load_states\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"\"\"send message to worker location\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# For backwards compatibility with Udacity course\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_handlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# TODO(karlhigley): Raise an exception if no handler is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/generic/abstract/message_handler.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouting_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_tensor_command\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_tensor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorCommandMessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPointerTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComputationAction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_computation_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_communication_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_computation_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;31m# TODO Andrew thinks this is gross, please fix. Instead need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/pipeline/subpipeline.py\u001b[0m in \u001b[0;36mload_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpipe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactories\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/tokenizer.py\u001b[0m in \u001b[0;36mload_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Create the query. This is the ID according to which the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/vocab.py\u001b[0m in \u001b[0;36mload_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatePointer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# Get a copy of the state using its pointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/pointers/state_pointer.py\u001b[0m in \u001b[0;36mget_copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0margs_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mkwargs_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"destination\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"\"\"send message to worker location\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# For backwards compatibility with Udacity course\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_handlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# TODO(karlhigley): Raise an exception if no handler is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/generic/abstract/message_handler.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouting_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_tensor_command\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_tensor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorCommandMessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPointerTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComputationAction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_computation_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_communication_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_computation_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0;31m# TODO Andrew thinks this is gross, please fix. Instead need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/SyferText/syfertext/state.py\u001b[0m in \u001b[0;36msend_copy\u001b[0;34m(self, destination)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Send the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseWorker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mStatePointer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36msend_obj\u001b[0;34m(self, obj, location)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 \u001b[0mreceive\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \"\"\"\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjectMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     def request_obj(\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"\"\"send message to worker location\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# For backwards compatibility with Udacity course\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m# Step 0: deserialize message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;31m# Step 1: save message and/or log it out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/serde/serde.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(binary, worker, strategy)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsgpack_deserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(binary, worker)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mworker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0msimple_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_msgpack_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deserialize_msgpack_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/serde/msgpack/serde.py\u001b[0m in \u001b[0;36m_deserialize_msgpack_binary\u001b[0;34m(binary, worker)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;31m# 1) Decompress the binary if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;31m# 2) Deserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deeplearning/openmined/PySyft/syft/serde/compression.py\u001b[0m in \u001b[0;36m_decompress\u001b[0;34m(binary)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# 1)  Decompress or return the original stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompress_scheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLZ4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlz4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcompress_scheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZLIB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    for iter, (vectors, targets) in enumerate(trainloader):\n",
    "        \n",
    "        # Set train mode\n",
    "        #classifier.train()\n",
    "\n",
    "        # Zero out previous gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Predict sentiment probabilities\n",
    "        probs, logits = classifier(vectors)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        loss = ((probs -  targets)**2).sum()\n",
    "\n",
    "\n",
    "        # Get the predicted labels\n",
    "        preds = probs.argmax(dim=1)\n",
    "        targets = targets.argmax(dim=1)\n",
    "        \n",
    "        # Compute the prediction accuracy\n",
    "        accuracy = (preds == targets).sum()\n",
    "        accuracy = accuracy.get().float_precision()\n",
    "        accuracy = 100 * (accuracy / batch_size)\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optim.step()\n",
    "\n",
    "        # Decrypt the loss for logging\n",
    "        loss = loss.get().float_precision()\n",
    "\n",
    "        \n",
    "        # Log to Tensorboard\n",
    "        writer.add_scalar('train/loss', loss, epoch * len(trainloader) + iter )\n",
    "        writer.add_scalar('train/acc', accuracy, epoch * len(trainloader) + iter )\n",
    "\n",
    "        \n",
    "        \"\"\" Perform validation on exactly one batch \"\"\"\n",
    "        \n",
    "        # Set validation mode\n",
    "        #classifier.eval()\n",
    "\n",
    "        for vectors, targets in valloader:\n",
    "            \n",
    "            probs, logits = classifier(vectors)\n",
    "\n",
    "            loss = ((probs -  targets)**2).sum()\n",
    "\n",
    "            preds = probs.argmax(dim=1)\n",
    "            targets = targets.argmax(dim=1)\n",
    "\n",
    "            accuracy = preds.eq(targets).sum()\n",
    "            accuracy = accuracy.get().float_precision()\n",
    "            accuracy = 100 * (accuracy / batch_size)\n",
    "\n",
    "            loss = loss.get().float_precision()\n",
    "            \n",
    "            \n",
    "            # Log to tensorboard\n",
    "            writer.add_scalar('val/loss', loss, epoch * len(trainloader) + iter )\n",
    "            writer.add_scalar('val/acc', accuracy, epoch * len(trainloader) + iter )\n",
    "            \n",
    "            break\n",
    "\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add the trained classifier to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classifier should be bundled in a SyferText object called \n",
    "# `SingleLabelClassifier`\n",
    "from syfertext.pipeline.single_label_classifier import SingleLabelClassifier\n",
    "from syfertext.pipeline.average_doc_encoder import AverageDocEncoder\n",
    "\n",
    "# Create a document encoder\n",
    "average_doc_encoder = AverageDocEncoder()\n",
    "\n",
    "# Bundle the trained classifier\n",
    "slc = SingleLabelClassifier(doc_encoder= average_doc_encoder,\n",
    "                            classifier = classifier,\n",
    "                            encryption = 'mpc',\n",
    "                            labels = ['negative', 'positive']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then Add it to the pipeline\n",
    "nlp_sentiment.add_pipe(component=slc,\n",
    "                       name = 'sentiment_classifier',\n",
    "                       access = {'*'}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tokenizer', 'class_name': 'Tokenizer'},\n",
       " {'name': 'stop tagger', 'class_name': 'SimpleTagger'},\n",
       " {'name': 'polarity tagger', 'class_name': 'SimpleTagger'},\n",
       " {'name': 'sentiment_classifier', 'class_name': 'SingleLabelClassifier'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment.pipeline_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenizer': {'state': State>None, 'access': {'*'}},\n",
       " 'vocab': {'state': State>None, 'access': {'*'}},\n",
       " 'stop tagger': {'state': State>None, 'access': {'*'}},\n",
       " 'polarity tagger': {'state': State>None, 'access': {'*'}},\n",
       " 'sentiment_classifier': {'state': State>None, 'access': {'*'}}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment.states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deploy the pipeline to PyGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentiment.deploy(worker=dan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syfertext_pricon_test1:tokenizer         |   State>None\n",
      "syfertext_pricon_test1:animal tagger     |   State>None\n",
      "syfertext_sentiment:tokenizer            |   State>None\n",
      "syfertext_sentiment:vocab                |   State>None\n",
      "syfertext_sentiment:stop tagger          |   State>None\n",
      "syfertext_sentiment:polarity tagger      |   State>None\n",
      "syfertext_sentiment:sentiment_classifier |   State>None\n",
      "syfertext_sentiment                      |   Pipeline>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(dan, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_object_store(me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load the pipeline and use it for * Local * inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentiment = syfertext.load(pipeline_name='syfertext_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'polarity tagger': {'access': {'*'},\n",
      "                     'location_id': 'dan',\n",
      "                     'state': State>None},\n",
      " 'sentiment_classifier': {'access': {'*'},\n",
      "                          'location_id': 'dan',\n",
      "                          'state': State>None},\n",
      " 'stop tagger': {'access': {'*'}, 'location_id': 'dan', 'state': State>None},\n",
      " 'tokenizer': {'access': {'*'}, 'location_id': 'dan', 'state': State>None},\n",
      " 'vocab': {'access': {'*'}, 'location_id': 'dan', 'state': State>None}}\n"
     ]
    }
   ],
   "source": [
    "pprint(nlp_sentiment.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer', 'stop tagger', 'polarity tagger', 'sentiment_classifier']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syfertext_sentiment                      |   Pipeline>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(me, 'syfertext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify a local string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alan/deeplearning/openmined/PySyft/syft/frameworks/torch/tensors/interpreters/additive_shared.py:122: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_sentiment('This is a really good movie. Watch it as soon as possible!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.syfertext_sentiment__sentiment_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This       | is_stop? True   is_polar? False\n",
      "is         | is_stop? True   is_polar? False\n",
      "a          | is_stop? True   is_polar? False\n",
      "really     | is_stop? True   is_polar? False\n",
      "good       | is_stop? False  is_polar? False\n",
      "movie      | is_stop? False  is_polar? False\n",
      ".          | is_stop? False  is_polar? False\n",
      "Watch      | is_stop? False  is_polar? False\n",
      "it         | is_stop? True   is_polar? False\n",
      "as         | is_stop? True   is_polar? False\n",
      "soon       | is_stop? False  is_polar? False\n",
      "as         | is_stop? True   is_polar? False\n",
      "possible   | is_stop? False  is_polar? False\n",
      "!          | is_stop? False  is_polar? False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text.ljust(10), '|',\n",
    "          f\"is_stop? {token._.is_stop}\".ljust(15),\n",
    "          f\"is_polar? {token._.is_polar}\".ljust(15),\n",
    "\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_sentiment('This movie is aweful! Do not watch it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.syfertext_sentiment__sentiment_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This       | is_stop? True   is_polar? False\n",
      "movie      | is_stop? False  is_polar? False\n",
      "is         | is_stop? True   is_polar? False\n",
      "aweful     | is_stop? False  is_polar? True \n",
      "!          | is_stop? False  is_polar? False\n",
      "Do         | is_stop? True   is_polar? False\n",
      "not        | is_stop? True   is_polar? False\n",
      "watch      | is_stop? False  is_polar? False\n",
      "it         | is_stop? True   is_polar? False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text.ljust(10), '|',\n",
    "          f\"is_stop? {token._.is_stop}\".ljust(15),\n",
    "          f\"is_polar? {token._.is_polar}\".ljust(15),\n",
    "\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DocPointer' object has no attribute '_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-89f1c7349233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyfertext_sentiment__sentiment_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DocPointer' object has no attribute '_'"
     ]
    }
   ],
   "source": [
    "text = String(\"This movie is really good!\").send(alice)\n",
    "\n",
    "doc = nlp_sentiment(text)\n",
    "\n",
    "doc._.syfertext_sentiment__sentiment_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syfertext.pointers.doc_pointer.DocPointer'>\n"
     ]
    }
   ],
   "source": [
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'me': [SubPipeline[tokenizer > stop tagger > polarity tagger > sentiment_classifier]],\n",
       " 'alice': [[SubPipelinePointer | me:5361953219 -> alice:79505398423]]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Create/Load the pipeline and use it for * Remote * inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the new pipeline\n",
    "nlp_sentiment_remote = syfertext.create('syfertext_sentiment_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "nlp_sentiment_remote.set_tokenizer(tokenizer = tokenizer, \n",
    "                             access= {'*'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add a vocab from 'spacy_en_core_web_lg_reduced'\n",
    "nlp_sentiment_remote.set_vocab(vocab=vocab_spacy, access={'*'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the stop word to the pipeline\n",
    "nlp_sentiment_remote.add_pipe(name = 'stop tagger',\n",
    "                        component = stop_tagger,\n",
    "                        access = {'*'}\n",
    "                       )\n",
    "\n",
    "# Add the polarity tagger to the pipeline\n",
    "nlp_sentiment_remote.add_pipe(name = 'polarity tagger',\n",
    "                       component = polarity_tagger,\n",
    "                       access = {'*'}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle the trained classifier\n",
    "slc = SingleLabelClassifier(doc_encoder= average_doc_encoder,\n",
    "                            classifier = classifier,\n",
    "                            encryption = 'mpc',\n",
    "                            labels = ['negative', 'positive']\n",
    "                           )\n",
    "\n",
    "# Then Add it to the pipeline\n",
    "nlp_sentiment_remote.add_pipe(component=slc,\n",
    "                       name = 'sentiment_classifier',\n",
    "                       access = {\"dan\"}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentiment_remote.deploy(worker=dan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "syfertext_pricon_test1:tokenizer         |   State>None\n",
      "syfertext_pricon_test1:animal tagger     |   State>None\n",
      "syfertext_sentiment_2:sentiment_classifier |   State>None\n",
      "syfertext_sentiment_2:tokenizer          |   State>None\n",
      "syfertext_sentiment_2:vocab              |   State>None\n",
      "syfertext_sentiment_2:stop tagger        |   State>None\n",
      "syfertext_sentiment_2:polarity tagger    |   State>None\n",
      "syfertext_sentiment_2                    |   Pipeline>None\n"
     ]
    }
   ],
   "source": [
    "show_pipeline_objects(dan, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_object_store(me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sentiment_remote = syfertext.load(pipeline_name='syfertext_sentiment_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syfertext.pointers.doc_pointer.DocPointer"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = String('I love text analysis').send(bob)\n",
    "\n",
    "doc = nlp_sentiment_remote(text)\n",
    "\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syfertext.pointers.doc_pointer.DocPointer"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = String('I love text analysis').send(alice)\n",
    "\n",
    "doc = nlp_sentiment_remote(text)\n",
    "\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alice': [[SubPipelinePointer | me:56152801768 -> alice:29528484018],\n",
      "           [SubPipelinePointer | me:97549406075 -> dan:91961013603]],\n",
      " 'bob': [[SubPipelinePointer | me:83809813079 -> bob:89639246577],\n",
      "         [SubPipelinePointer | me:97549406075 -> dan:91961013603]]}\n"
     ]
    }
   ],
   "source": [
    "pprint(nlp_sentiment_remote.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'alice': [{'location': 'alice',\n",
      "                        'names': ['tokenizer',\n",
      "                                  'stop tagger',\n",
      "                                  'polarity tagger']},\n",
      "                       {'location': 'dan', 'names': ['sentiment_classifier']}],\n",
      "             'bob': [{'location': 'bob',\n",
      "                      'names': ['tokenizer', 'stop tagger', 'polarity tagger']},\n",
      "                     {'location': 'dan', 'names': ['sentiment_classifier']}]})\n"
     ]
    }
   ],
   "source": [
    "pprint(nlp_sentiment_remote.subpipeline_templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Moving to the real grid. (A big part is done!)\n",
    "\n",
    "2. Working with real dataset formats hosted on PyGrid\n",
    "\n",
    "3. Working on real-world result delivery scenarios and formats. How to consume the DocPointer.\n",
    "\n",
    "4. Expand the SyferText model zoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
